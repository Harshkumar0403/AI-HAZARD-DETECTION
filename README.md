# Food Safety Hazard & Product Category Classification using BERT

This repository presents a complete NLP pipeline to automatically classify food safety incident reports into:

- **Product Categories** (e.g., dairy, meat, beverages, etc.)
- **Hazard Types** (e.g., biological, allergens, chemical, etc.)

The pipeline leverages state-of-the-art transformer models (BERT), includes preprocessing, data augmentation, model training, evaluation, and prediction generation.

---

## ğŸ§¾ Problem Statement

The goal is to build an automated text classification system that can extract meaningful labels from incident reports related to food safety. Given a report's `title` and `text`, the system predicts:

- **Hazard Type** â€“ The nature of the risk involved (e.g., allergens, biological).
- **Product Category** â€“ The category of food product associated with the hazard (e.g., meat, dairy).

This helps food regulatory agencies streamline monitoring and response.

---

## ğŸ“„ Dataset Overview

Two datasets are used:

- `final_df.csv` â€“ Labeled training and validation dataset
- `test_df.csv` â€“ Unlabeled test set for final predictions

Each report contains:
- `ID`
- `title`
- `text`
- `hazard-type` (for final_df)
- `product-category` (for final_df)

---

## âš™ï¸ Pipeline Summary

### ğŸ”¹ Data Preprocessing
- Combined `title` and `text` into one field
- Synonym-based text augmentation using **WordNet**
- Label encoding for classification targets
- Class balancing using computed weights
- Tokenization using `bert-base-uncased`
- Format conversion to HuggingFace `Dataset`

### ğŸ”¹ Model Training
- Model: `BertForSequenceClassification`
- Metrics: Accuracy and Weighted F1 Score
- Trained using HuggingFaceâ€™s `Trainer` class
- Separate models trained for:
  - `product-category`
  - `hazard-type`

---

## ğŸ“Š Results & Performance

### âœ… Product Category Classification

| Metric         | Score |
|----------------|-------|
| Accuracy       | 0.90  |
| Macro F1 Score | 0.84  |
| Weighted F1    | 0.90  |

The model performs consistently across most categories with strong F1 scores. Smaller classes show slight performance dips due to data imbalance.

---

### âœ… Hazard Type Classification

| Metric         | Score |
|----------------|-------|
| Accuracy       | 0.95  |
| Macro F1 Score | 0.92  |
| Weighted F1    | 0.95  |

The model demonstrates excellent performance across all hazard classes, including strong results even in low-support categories.

---

## ğŸ§ª How to Use

### ğŸ”§ Local Prediction

To run local predictions on the test set:

1. Place test data in:
./test_dataset/Hazards_UNLABELLED_TEST.csv

2. Ensure model files are saved under:
   ./saved_model/product-category/
   ./saved_model/hazard-type/

4. Run the main prediction script:

```bash
python main.py

## Install necessary Python packages with:
~ pip install -r requirements.txt

### Project structure

.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ final_df.csv
â”‚   â””â”€â”€ test_df.csv
â”œâ”€â”€ saved_model/
â”‚   â”œâ”€â”€ product-category/
â”‚   â””â”€â”€ hazard-type/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ model_training.ipynb
â”œâ”€â”€ results/
â”‚   â””â”€â”€ pred.csv
â”œâ”€â”€ main.py
â””â”€â”€ README.md



